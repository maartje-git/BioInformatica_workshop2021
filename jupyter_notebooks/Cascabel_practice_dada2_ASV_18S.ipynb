{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Workshop 2021\n",
    "### Cascabel_S5_practice\n",
    "### Running Cascabel pipeline with DADA2 (ASV)\n",
    "### NIOZ101 18S BlackSea2013\n",
    "#### Maartje Brouwer\n",
    "#### 2021 Feb 15\n",
    "###### This notebook is running Python 3.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/lv3/scratch/workshop_2021/Users/maartje/BlackSea_2013/CASCABEL\n"
     ]
    }
   ],
   "source": [
    "# Go to the correct directory\n",
    "%cd /export/lv3/scratch/workshop_2021/Users/maartje/BlackSea_2013/CASCABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34mAnaconda is now active on server ada with base /opt/biolinux/anaconda3          python version : 3.6.7.final.0\r\n",
      "Run \"conda deactivate\" command to deactivate and exit current python environment\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# activate conda, qiime, correct version of R\n",
    "! source ~/.bashrc.conda3\n",
    "! source activate qiime1\n",
    "! export PATH=$PATH:/opt/biolinux/anaconda3/bin\n",
    "! module load R/4.0.3\n",
    "! unset LD_PRELOAD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration files\n",
    "As you can see, there are different configuration files on the CASCABEL directory. These configuration files, are already preconfigured for some specific workflows\n",
    "* config.asv.double_bc.yaml  ASV workflow for paired end data\n",
    "* config.asv.double_bc.unpaired.yaml ASV workflow for paired end data and “unpaired” reads.\n",
    "* config.asv.single_bc.yaml ASV workflow for single end data\n",
    "* config.otu.double_bc.yaml OTU workflow for paired end data\n",
    "* config.otu.double_bc.unpaired.yaml OTU workflow for paired end data and “unpaired” reads.\n",
    "\n",
    "First copy the config file and name it  \n",
    "`! cp config.asv.double_bc.yaml config.asv.double_bc.NIOZ101.yaml`  \n",
    "Then load the new file  \n",
    "`%load cconfig.asv.double_bc.NIOZ101.yaml`  \n",
    "Then write `%%writefile config.asv.double_bc.NIOZ101.yaml` command on top  \n",
    "edit file and run the `%%writefile` command\n",
    "\n",
    "[databases can be found here] (http://redmine.nioz.nl/projects/pipeline-for-amplicon-analysis/wiki/Run#9-Database)\n",
    "\n",
    "Dada2 requires clean fastq files per sample. With “clean” we mean, no barcodes and no primers. In the previous OTU run, these files were already generated, so we are going to take advantage of that, that is why we “re-use” the same PROJECT, and the same RUN. Cascabel will not overwrite the results for different workflows! And you will have a different report at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.asv.double_bc.NIOZ101.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.asv.double_bc.NIOZ101.yaml\n",
    "# %load config.asv.double_bc.NIOZ101.yaml\n",
    "################################################################################\n",
    "#                             CONFIGURATION FILE                               #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Configuration file for the CASCABEL pipeline.                                #\n",
    "# Set the parameters below, save the file and run Snakemake.                   #\n",
    "# The file format is yaml (http://www.yaml.org/). In this file, you specify    #\n",
    "# your input data, barcode mapping file and you can choose tools and parameters#\n",
    "# according to your needs. Most rules and parameters have default settings.    #\n",
    "# It is very important to keep the indentation of the file (don’t change the   #\n",
    "# tabs and spaces), as well as the name of the parameters/variables. But you   #\n",
    "# can of course change the values of the parameters to deviate from the default# \n",
    "# settings. Any text after a hashtag (#) is considered a comment and will be   #\n",
    "# ignored by Snakemake.                                                        #\n",
    "#                                                                              #\n",
    "# @Author: Julia Engelmann and Alejandro Abdala                                #\n",
    "# @Last update: 16/11/2020                                                     #\n",
    "################################################################################\n",
    "\n",
    "################################################################################\n",
    "#                       GENERAL PARAMETERS SECTION                             # \n",
    "#------------------------------------------------------------------------------#\n",
    "# The general parameters section defines parameters that are global or general #\n",
    "# for the complete workflow.                                                   #\n",
    "################################################################################\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                                Execution mode                                #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This parameter allows the user to inspect intermediate files in order to     #\n",
    "# finetune some downstream analyses, re-do previous steps or exit the workflow.#\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\t            \n",
    "# -interactive   Set this flag to \"T\" (default) in order to interact at some   #\n",
    "#                specific steps with the pipeline. \"F\" will try to run all the #\n",
    "#                pipeline without communicating intermediate results until the #\n",
    "#                report.                                                       # \n",
    "# For a list for all the interactive checkpoints take a look at the following  #\n",
    "# link: https://github.com/AlejandroAb/CASCABEL/wiki#5-interactive-mode        #\n",
    "#------------------------------------------------------------------------------#\n",
    "interactive : \"T\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                             Project Name                                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "# The name of the project for which the pipeline will be executed. This should #\n",
    "# be the same name used as the first parameter with the init_sample.sh script  #\n",
    "# (if used for multiple libraries).                                            #\n",
    "#------------------------------------------------------------------------------#\n",
    "PROJECT: \"BlackSea2016\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                            LIBRARIES/SAMPLES                                 #\n",
    "#------------------------------------------------------------------------------#\n",
    "# SAMPLES/LIBRARIES you want to include in the analysis.                       #\n",
    "# Use the same library names as with the init_sample.sh script.                #\n",
    "# Include each library name surrounded by quotes, and comma separated.         #\n",
    "# i.e LIBRARY:  [\"LIB_1\",\"LIB_2\",...\"LIB_N\"]                                   #\n",
    "# LIBRARY_LAYOUT: Configuration of the library; all the libraries/samples      #\n",
    "#                 must have the same configuration; use:                       #\n",
    "#                 \"PE\" for paired-end reads [Default].                         #\n",
    "#                 \"SE\" for single-end reads.                                   #\n",
    "#------------------------------------------------------------------------------#\n",
    "LIBRARY: [\"NIOZ101\"]\n",
    "LIBRARY_LAYOUT: \"PE\"\n",
    "#------------------------------------------------------------------------------#\n",
    "#                               RUN                                            #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Name of the RUN - Only use alphanumeric characters and don't use spaces.     #\n",
    "# This parameter helps the user to execute different runs (pipeline executions)#\n",
    "# with the same input data but with different parameters (ideally).            #\n",
    "# The RUN parameter can be set here or remain empty, in the latter case, the   #\n",
    "# user must assign this value via the command line.                            #\n",
    "# i.e:  --config RUN=run_name                                                  #\n",
    "#------------------------------------------------------------------------------#\n",
    "RUN: \"18S\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                           Description                                        #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Brief description of the run. Any description written here will be included  #\n",
    "# in the final report. This field is not mandatory so it can remain empty.     #\n",
    "#------------------------------------------------------------------------------#\n",
    "description: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                             INPUT TYPE                                       #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Cascabel supports two types of input files, fastq and gzipped fastq files.   #     \n",
    "# This parameter can take the values \"T\" if the input files are gzipped        #\n",
    "# (only the reads!, the metadata file always needs to be uncompressed) or \"F\"  #\n",
    "# if the input files are regular fastq files.                                  #\n",
    "#------------------------------------------------------------------------------#\n",
    "gzip_input: \"F\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                             INPUT FILES                                      #\n",
    "#------------------------------------------------------------------------------#\n",
    "# To run Cascabel for multiple libraries you can provide an input file, tab    #\n",
    "# separated with the following columns:                                        #\n",
    "# - Library: Name of the library (this have to match with the values entered   #\n",
    "#            in the LIBRARY variable described above).                         #\n",
    "# - Forward reads: Full path to the forward reads.                             #\n",
    "# - Reverse reads: Full path to the reverse reads (only for paired-end).       #\n",
    "# - metadata:      Full path to the file with the information for              #\n",
    "#                  demultiplexing the samples (only if needed).                #\n",
    "# The full path of this file should be supplied in the input_files variable,   #\n",
    "# otherwise, you have to enter the FULL PATH for both: the raw reads and the   #\n",
    "# metadata file (barcode mapping file). The metadata file is only needed if    #\n",
    "# you want to perform demultiplexing.                                          #\n",
    "# If you want to avoid the creation of this file a third solution is available #\n",
    "# using the script init_sample.sh. More info at the project Wiki:              #\n",
    "# https://github.com/AlejandroAb/CASCABEL/wiki#21-input-files                  #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - fw_reads:  Full path to the raw reads in forward direction (R1)            #\n",
    "# - rw_reads:  Full path to the raw reads in reverse direction (R2)            #\n",
    "# - metadata:  Full path to the metadata file with barcodes for each sample    #\n",
    "#              to perform library demultiplexing                               #\n",
    "# - input_files: Full path to a file with the information for the library(s)   #\n",
    "#                                                                              #\n",
    "# ** Please supply only one of the following:                                  #\n",
    "#     - fw_reads, rv_reads and metadata                                        #\n",
    "#     - input_files                                                            #\n",
    "#     - or use init_sample.sh script directly                                  #\n",
    "#------------------------------------------------------------------------------#\n",
    "fw_reads: \"/export/lv3/scratch/workshop_2021/Users/maartje/BlackSea_2013/Data/RawData/NIOZ101_1.fastq\"\n",
    "rv_reads: \"/export/lv3/scratch/workshop_2021/Users/maartje/BlackSea_2013/Data/RawData/NIOZ101_2.fastq\"\n",
    "metadata: \"/export/lv3/scratch/workshop_2021/Users/maartje/BlackSea_2013/ProjectInfo/NIOZ101/NIOZ101_18S_blackSea.txt\"\n",
    "#OR \n",
    "input_files: \"\"\n",
    "\n",
    "################################################################################\n",
    "#                          REPORT PARAMETER SECTION                            #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This section defines parameters that will influence the type of report to be #\n",
    "# generated at the end of the workflow.                                        #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                           PDF Report                                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# By default, Cascabel creates the final report in HTML format. In order to    #\n",
    "# create the report also as pdf file, set this flag to \"T\".                    #\n",
    "# Important! in order to convert the file to pdf format, it is necessary to    #\n",
    "# execute the pipeline within a Xserver session i.e MobaXterm or ssh -X.       #\n",
    "# One way to validate if your active session is using an Xserver, execute      #\n",
    "# 'echo $DISPLAY'  on a command line terminal. If this returns empty, you do   #\n",
    "# not have an Xserver session.                                                 #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - pdfReport            \"T\" for generate pdf report or \"F\" to skip it.        #\n",
    "# - wkhtmltopdf_command  name w/wo path to execute the html to pdf translation.#\n",
    "#------------------------------------------------------------------------------#\n",
    "pdfReport: \"T\"\n",
    "wkhtmltopdf_command: \"wkhtmltopdf  -T 10mm -B 30mm\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                           Portable Report                                    #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Cascabel creates the final report in HTML format, containing references to   #\n",
    "# other images or links. Therefore just copying the HTML files for sharing or  #\n",
    "# inspecting the results will break these links.                               #\n",
    "# By setting 'portableReport' to true \"T\", CASCABEL will generate a zip file   #\n",
    "# with all the resources necessary to share and distribute CASCABEL's report.  #\n",
    "#------------------------------------------------------------------------------#\n",
    "portableReport: \"T\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                           Krona Report                                       #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Krona allows hierarchical data to be explored with zooming, multi-layered pie#\n",
    "# charts. The interactive charts are self-contained and can be viewed with any #\n",
    "# modern web browser.                                                          #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - report        Indicate with \"T\"/\"F\" if CASCABEL should generate a Krona    #\n",
    "#                 chart.                                                       #\n",
    "# - ktImportText  Name of the command to invoke this krona utility.            #\n",
    "# - samples       Indicate the samples to be included in the chart, use comma  #\n",
    "#                 separated values of samples (same name as the ones supplied  #\n",
    "#                 in the metadata barcode file). Or \"all\" to include all the   #\n",
    "#                 samples.                                                     #\n",
    "# - otu_table     Target OTU table for the report. Use \"default\" to use the    #\n",
    "#                 filtered OTU table (exclude singletons). Or \"singletons\" to  #\n",
    "#                 use the non filtered OTU table (include singletons)          #  \n",
    "# - extra_params  Any other extra parameter from ktImportText tool. default    #\n",
    "#                 \"-n root_extra\"                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "krona:\n",
    "  report: \"T\"\n",
    "  command: \"ktImportText\"\n",
    "  samples: \"all\"\n",
    "  otu_table: \"default\"\n",
    "  extra_params: \"-n root_extra\"\n",
    "\n",
    "################################################################################\n",
    "#                        Specific Parameters Section                           #\n",
    "#------------------------------------------------------------------------------#\n",
    "# In this section of the configuration file, you can find all the parameters   #\n",
    "# used to run the different rules during the execution of the pipeline.        #\n",
    "# Some of the entries below contain a parameter called \"extra_params\".         #\n",
    "# This parameter is designed to allow the user to pass any other extra         #\n",
    "# parameter to the program invoked by the rule, as some rules do not list all  #\n",
    "# the parameters of the underlying tool explicitly. In these cases, the user   #\n",
    "# can specify any other parameter using \"extra_params\".                        #\n",
    "# IMPORTANT NOTE:                                                              #\n",
    "# After defining the type of analysis, in the header of the comments for each  #\n",
    "# set of parameters, you will see a prefix indicating if the parameters/options# \n",
    "# apply for the OTU workflow, the ASV workflow or both. For more information in# \n",
    "# the type of analyses, please refer to the next section \"ANALYSIS TYPE\".      #\n",
    "################################################################################\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "#                                 ANALYSIS TYPE                                #\n",
    "# rules:                                                                       #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Cascabel supports two main types of analysis:                                #\n",
    "#  1) Analysis based on traditional OTUs (Operational Taxonomic Units) which   #\n",
    "#     are mostly generated by clustering sequences based on a shared           #\n",
    "#     similarity threshold.                                                    #\n",
    "#  2) Analysis based on ASVs (Amplicon sequence variants). This kind of        #\n",
    "#     analysis tries to distinguish errors in the sequence reads from true     #\n",
    "#     sequence variants, down to the level of single-nucleotide differences.   #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              # \n",
    "# - ANALYSIS_TYPE    \"OTU\" or \"ASV\". Defines the type analysis                 #\n",
    "#------------------------------------------------------------------------------#\n",
    "ANALYSIS_TYPE: \"ASV\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                  UNPAIRED DATA WORK FLOW                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "# A regular workflow for marker gene analysis using paired-end data,           #\n",
    "# comprehends the merging of forward and reverse reads, prior to continuing    #\n",
    "# with downstream analysis implemented within this pipeline.   \t  \t           #\n",
    "# However, primers can intentially amplify fragments which are so large that   #\n",
    "# forward and reverse reads do not overlap. A regular analysis would discard   #\n",
    "# all those \"unpaired\" reads during the FW and RV read assembly. For this      #\n",
    "# scenario, Cascabel implements an alternative flow, where instead of          #\n",
    "# continuing with assembled reads, un-assembled reads are concatenated together# \n",
    "# with a degenerated base 'N' between the FW read and the reverse complemented #\n",
    "# RV read (which does not significantly influence k-mer based classification   #\n",
    "# methods such as RDP).                                                        #                                                                             #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "#                                                                              # \n",
    "# - UNPAIRED_DATA_PIPELINE    \"T\" or \"F\". True to work with the \"un-assembled\" #\n",
    "#                             reads. If ANALYSIS_TYPE = \"ASV\" this option use  #\n",
    "#                             the \"justConcatenate\" option from the            #\n",
    "#                             mergePairs() function from the dada2 package.    #\n",
    "# - CHAR_TO_PAIR              In-silico base used to pair both fragments.      #\n",
    "#                             Valid values A|T|G|C|N. You can use more than one#\n",
    "#                             base, e.g. \"GGGGG\" pair with 5 Gs.               #\n",
    "#                             This parameter only applies for the OTU-WF. For  #\n",
    "#                             the ASV-WF, dada2 merge both reads with 10 Ns.   # \n",
    "# - QUALITY_CHAR              The forward and reverse reads are paired         #\n",
    "#                             in-silico in a fastq file, thus the quality for  #\n",
    "#                             the pairing bases must be provided. If more than #\n",
    "#                             one CHAR_TO_PAIR is used, you only need to       #\n",
    "#                             specify one and only one QUALITY_CHAR.           #\n",
    "#                             This parameter only applies for the OTU-WF. For  #\n",
    "#                             the ASV-WF, dada2 merge both reads with 10 Ns.   #  \n",
    "#------------------------------------------------------------------------------#\n",
    "UNPAIRED_DATA_PIPELINE: \"F\"\n",
    "CHAR_TO_PAIR: \"T\"\n",
    "QUALITY_CHAR: \"G\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:               Quality control with FastQC                           #\n",
    "# rules: fast_qc, validateQC                                                   #\n",
    "#------------------------------------------------------------------------------#\n",
    "# FastQC evaluates 12 main concepts on the sequences: basic statistics, per    #\n",
    "# base sequence quality, per tile sequence quality, per sequence quality       #\n",
    "# scores, per base sequence content, per sequence GC content, per base N       #\n",
    "# content, sequence length distribution, sequence duplication levels,          #\n",
    "# overrepresented sequences, adapter content and Kmer content.                 #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - command       Command needed to invoke fastqc [default: \"fastqc\"].         #\n",
    "# - extra_params  Extra parameters. See fastqc --help for more info.           #\n",
    "# - qcLimit       Set the maximum number of FastQC FAILS (from the 12 tests    #\n",
    "#                 evaluated) accepted before interrupting the workflow if the  #\n",
    "#                 'interactive' mode is equal to \"T\".                          #\n",
    "#------------------------------------------------------------------------------#\n",
    "fastQC:\n",
    "  command: \"fastqc\"\n",
    "  extra_params: \"\"\n",
    "  qcLimit : 3\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:       Assemble fragments (merge forward with reverse reads)         #\n",
    "# rule: pear                                                                   #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This step is performed to merge paired reads.                                #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - t             Minimum length of reads after trimming low quality bases.    #\n",
    "# - v             Minimum overlap size. The minimum overlap may be set to 1    #\n",
    "#                 when the statistical test is used. (default in pear: 10)     #\n",
    "# - j             Number of threads to use.                                    #\n",
    "# - p             The p-value cutoff used in the statistical test. Valid       #\n",
    "#                 options are: 0.0001, 0.001, 0.01, 0.05 and 1.0. Setting 1.0  #\n",
    "#                 disables the test. (default: 0.01)                           #\n",
    "# - extra_params  Extra parameters. See pear --help for more info.             #\n",
    "# - prcpear       The minimum percentage of expected paired reads, if the      #\n",
    "#                 actual percentage is lower and the 'interactive' parameter   #\n",
    "#                 is set to \"T\"; a warning message will be shown.              #\n",
    "#------------------------------------------------------------------------------#\n",
    "pear:\n",
    "  command: \"pear\"\n",
    "  t: 100\n",
    "  v: 10\n",
    "  j: 6\n",
    "  p: 0.05\n",
    "  extra_params: \"\"\n",
    "  prcpear: 90\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:         FastQC on merged/assembled fragments                        #\n",
    "# rule: fastQCPear                                                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Once the paired-end reads have been merged into one fragment, run FastQC     #\n",
    "# again to check their quality. Set this option to \"T\" (true) or \"F\" (false)   #\n",
    "# in order to execute or skip this step.                                       #\n",
    "#------------------------------------------------------------------------------#\n",
    "fastQCPear: T\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                         QIIME                                       #\n",
    "# rule: bc_mapping_validation, extract_barcodes, extract_barcodes_unassigned,  #\n",
    "# split_libraries, split_libraries_rc, search_chimera, cluster_OTUs,           #\n",
    "# pick_representatives, assign_taxonomy, make_otu_table, summarize_taxa,       #\n",
    "# filter_rep_seqs, align_rep_seqs, filter_alignment, make_tree                 #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Different QIIME scripts are used along the pipeline and in order to execute  #\n",
    "# these scripts, they need to be located on the user PATH environmental        #\n",
    "# variable, or QIIME's bin directory needs to be included in the parameter:    #\n",
    "# 'path' below. This parameter will be used by the pipeline for all the rules  #\n",
    "# that use a Qiime script.                                                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "qiime:\n",
    "  path: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                           R                                         #\n",
    "# rules:  correct_barcodes, correct_barcodes_unassigned, histogram_chart       #\n",
    "#------------------------------------------------------------------------------#\n",
    "# R is used by different rules within the pipeline. In order to run these      #\n",
    "# rules, CASCABEL uses the Rscript command.                                    #\n",
    "# Here you can change the command to call Rscript.                             #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - command     Rscript command, [default \"Rscript\"].                          #\n",
    "#------------------------------------------------------------------------------#\n",
    "Rscript:\n",
    "  command: \"Rscript\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                         JAVA                                        #\n",
    "# rules:  write_dmx_files, degap_alignment, remap_clusters                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Java is used by different rules within the pipeline. In order to run java,   #\n",
    "# the pipeline needs to know how to invoke it.                                 #\n",
    "# Here you can change the command to invoke java.                              #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - command    java command and or path to binaries if needed [default: \"java\"]#\n",
    "#------------------------------------------------------------------------------#\n",
    "java:\n",
    "  command: \"java\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:          Demultiplex input files                                    #\n",
    "# rule: write_dmx_files                                                        #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Cascabel optionally performs library demultiplexing for barcoded reads.      #\n",
    "# This feature can be turned ON/OFF with the following options:                #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - demultiplex         \"T\". If the pipeline is going to demultiplex the input #\n",
    "#                       files. In this case, the metadata file has to be       #\n",
    "#                       provided.                                              #\n",
    "#                       \"F\". If the input files are already demultiplexed.     #\n",
    "# - create_fastq_files  \"T\" or \"F\". If 'demultiplex' = T and this is also T,   #\n",
    "#                       the pipeline will create demultiplexed fastq files per #\n",
    "#                       sample.                                                #\n",
    "# - remove_bc           The demultiplexed fastq files are created using the    #\n",
    "#                       raw data, thus they may contain artificial sequences   #\n",
    "#                       like the barcodes. This option trims the first N bases #\n",
    "#                       from each read.                                        #\n",
    "# - order_by_strand     During demultiplexing it is possible to identify the   #\n",
    "#                       current strand of the read according to the barcode.   #\n",
    "#                       Set this option to \"T\" in order to interchange FW with #\n",
    "#                       RV reads when barcodes are found on opposite strands.  #\n",
    "# - add_unpair          \"T\". Un-assembled reads are also included within the   #\n",
    "#                       demultiplexing process in order to being assigned to   #\n",
    "#                       their samples.                                         #\n",
    "#                       \"F\". Only carry out the demultiplexing process with    #\n",
    "# \t\t\t            paired reads.                              #\n",
    "# - dmx_params          Parameters to pass on to the demultiplexing script.    #\n",
    "#                       For example, the user can change the prefix and suffix #\n",
    "#                       of the output files. To see the available parameters,  #\n",
    "#                       run: java -cp Scripts DemultiplexQiime.                #\n",
    "# - primers             In the same sense as for \"remove_bc\" option,           #\n",
    "#                       demultiplexed reads may contain adapters or primers.   #\n",
    "#                       The following parameters supply options for removing   #\n",
    "#                       these sequences by using cutadapt.                     #  \n",
    "#   ** remove           \"T\" or \"F\" for removing or not any kind of adapter.    #\n",
    "#                       This option can be set to true (\"T\") even though the   #\n",
    "#                       \"demultiplex\" option is set to false (\"F\"). Useful when#\n",
    "#                       starting with demultiplexed reads containing adapters  #\n",
    "#                       for a ASV workflow.                                    #\n",
    "#   ** fw_primer        Sequence to be removed from forward reads. It accepts  #\n",
    "#                       IUPAC wildcards, e.g., \"^GTGYCAGCMGCCGCGGTAA\".         #\n",
    "#   ** rv_primer        Sequence to be removed from reverse reads. It accepts  #\n",
    "#                       IUPAC wildcards, e.g., \"^GGACTACNVGGGTWTCTAAT\".        #\n",
    "#   ** min_overlap      Minimum overlap between a primer and a sequence to be  #\n",
    "#                       identified.                                            #\n",
    "#   ** extra_params     Any extra parameter that you wish to supply to  the    #\n",
    "#                       cutadapt command. Default \"--discard-untrimmed\". With  #\n",
    "#                       this extra option, reads without the primers are       #\n",
    "#                       discarded.                                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "demultiplexing:\n",
    "  demultiplex: \"T\"\n",
    "  create_fastq_files: \"T\"\n",
    "  remove_bc: 12\n",
    "  order_by_strand: \"T\" \n",
    "  add_unpair: \"T\" \n",
    "  dmx_params: \"--remove-header\"\n",
    "  primers:\n",
    "    remove: \"F\"\n",
    "    fw_primer: \"\"\n",
    "    rv_primer: \"\"\n",
    "    min_overlap: \"5\"\n",
    "    extra_params: \"--discard-untrimmed --match-read-wildcards\"\n",
    "    \n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                  Extract barcodes                                   #\n",
    "# rules: extract_barcodes, extract_barcodes_unassigned                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule extract barcodes from the reads, and generates two files: one      #\n",
    "# with only the extracted barcodes and a second one with the sequences without #\n",
    "# the barcodes.                                                                #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - c             This parameter allows to choose the barcode configuration:   #\n",
    "#                 \"barcode_single_end\". The merged fragments start with the    #\n",
    "#\t\t          barcode sequence.                                            #\n",
    "#                 \"barcode_paired_stitched\". Input has barcodes at the         #\n",
    "#                 beginning and end of the merged fragment.                    #\n",
    "#                 \"barcode_paired_end\". This option is not valid here since    #\n",
    "#                 the reads have already been merged to one fragment.          #\n",
    "# - bc_length     If 'c' is \"barcode_paired_stitched\" use both:                #\n",
    "#                 --bc1_len X and --bc2_len Y.                                 #\n",
    "#                 If 'c' is \"barcode_single_end\" use only --bc1_len X.         #\n",
    "#                 X and Y are the nucleotide lengths of the barcodes.          #\n",
    "# - extra_params  Extra parameters. See extract_barcodes.py -help.             #\n",
    "#------------------------------------------------------------------------------#\n",
    "ext_bc:\n",
    "  c: \"barcode_paired_stitched\"\n",
    "  bc_length: \"--bc1_len 12 --bc2_len 12\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                    Barcode correction                               #\n",
    "# rules: correct_barcodes, correct_barcodes_unassigned                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This parameter allows error correction of barcodes. First, barcodes perfectly#\n",
    "# matching a sample barcode in the mapping file will be assigned to the        #\n",
    "# samples. If error correction is enabled, barcodes will be assigned to the    #\n",
    "# closest barcode with increasing number of mismatches until the maximum number# \n",
    "# of mismatches has been reached, e.g. if the mismatch is equal to \"2\", first  #\n",
    "# barcodes with one mismatch will be assigned, then barcodes with 2 mismatches.#\n",
    "#    bc_mismatch:  Number of allowed missmatches.                              # \n",
    "#                  bc_mismatch = 0: don't allow mismatches in the barcode.     #\n",
    "#                  bc_mismatch > 0: correct bc_mismatch bases at maximum.      #\n",
    "#------------------------------------------------------------------------------#\n",
    "bc_mismatch: 2\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                    Split libraries                                  #\n",
    "# rule: split_libraries, split_libraries_rc                                    #\n",
    "#------------------------------------------------------------------------------#\n",
    "# These rules performs demultiplexing of Fastq sequence data where barcodes and#\n",
    "# sequences are contained in two separate fastq files.                         #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - q             Maximum unacceptable Phred quality score. e.g., for Q20 and  #\n",
    "#                 better, specify -q 19). [default: 19]                        #\n",
    "# - r             Maximum number of consecutive low quality base calls allowed #\n",
    "#                 before truncating a read. [default: 3]                       #\n",
    "# - barcode_type  The type of barcode used. This can be an integer, e.g., \"6\"  #\n",
    "#                 or “golay_12” for golay error-correcting barcodes.           #\n",
    "# - extra_params  Any extra parameter. Run split_libraries_fastq -h to see all #\n",
    "#                 options.                                                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "split:\n",
    "  q: \"19\"\n",
    "  r: \"5\"\n",
    "  barcode_type:  \"24\"\n",
    "  extra_params: \" --phred_offset 33\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# ASV_WF:                   dada2 trim and filter reads                        #\n",
    "# rule: dada2Filter                                                            #\n",
    "#------------------------------------------------------------------------------#\n",
    "# These parameters take effect during the quality trimming and filtering steps #\n",
    "# implemented within dada2 using the filterAndTrim() function.                 #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - generateQAplots   Cascabel already generates FastQC reports, however dada2 #\n",
    "#                     quality plots can be generated by passing \"T\".           #\n",
    "#                     Default \"F\".                                             #\n",
    "# - truncFW           Truncate forward reads after truncFW bases. Reads shorter# \n",
    "#                     than this are discarded. Your reads must still overlap   #\n",
    "#                     after truncation in order to merge them later.           #      \n",
    "# - tuncRV            Truncate reverse reads after truncRV bases. Reads shorter# \n",
    "#                     than this are discarded. Your reads must still overlap   #\n",
    "#                     after truncation in order to merge them later.           #\n",
    "# - maxEE_FW          After truncation, forward reads with higher than         # \n",
    "#                     maxEE_FW \"expected errors\" will be discarded.            #\n",
    "# - maxEE_RV          After truncation, reverse reads with higher than         # \n",
    "#                     maxEE_RV \"expected errors\" will be discarded.            #\n",
    "# - cpus              Number of threads|cpus to be used.                       #\n",
    "# - extra_params      Any extra parameter belonging to dada2's function        #\n",
    "#                     filterAndTrim(). The value passed through this variable  #\n",
    "#                     is send directly to the function in R. Therefore, if your#\n",
    "#                     extra_params involves more than one argument, separate   #\n",
    "#                     them with commas i.e. Suppose you want to pass truncQ=2  #\n",
    "#                     and rm.phix=TRUE arguments. In R the function may look   #\n",
    "#                     like this: filterAndTrim(...,truncQ=2, rm.phix=TRUE)     #\n",
    "#                     Thus, the extra_params should look like the following:   #  \n",
    "#                     \"truncQ=2, rm.phix=TRUE\".                                #\n",
    "# Note from dada2's tutorial:                                                  #\n",
    "# \"The standard filtering parameters are starting points, not set in stone.    #\n",
    "# If you want to speed up downstream computation, consider tightening maxEE.   #\n",
    "# If too few reads are passing the filter, consider relaxing maxEE, perhaps    #\n",
    "# especially on the reverse reads (maxEE_RV), and reducing the truncLen to     #\n",
    "# remove low quality tails. Remember though, when choosing truncLen for        #\n",
    "# paired-end reads you must maintain overlap after truncation in order to      #\n",
    "# merge them later.\"                                                           #\n",
    "#------------------------------------------------------------------------------# \n",
    "dada2_filter:\n",
    "  generateQAplots: \"T\"\n",
    "  truncFW: 250\n",
    "  truncRV: 240\n",
    "  maxEE_FW: 3\n",
    "  maxEE_RV: 5\n",
    "  cpus: 10\n",
    "  extra_params: \"\\\",truncQ=2, rm.phix=TRUE\\\"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# ASV_WF:                          dada2 merge pairs                           #\n",
    "# rule: run_dada2                                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "# After denoising forward and reverse reads, they are assembled with the       #\n",
    "# mergePairs() function from dada2 package.                                    #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - minOverlap      Default 12. The minimum length of the overlap required for #\n",
    "#                   merging the forward and reverse reads.                     #\n",
    "# - maxMismatch     Default 0. The maximum mismatches allowed in the overlap   #\n",
    "#                   region.                                                    #\n",
    "#------------------------------------------------------------------------------#\n",
    "dada2_merge:\n",
    "  minOverlap: 10\n",
    "  maxMismatch: 0\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# ASV_WF:                           dada2 generates ASV                        #\n",
    "# rule: run_dada2                                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Find true Amplicon Sequence Variants by denoising sequences with the dada()  #\n",
    "# function.                                                                    #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - generateErrPLots    Default \"F\". Use \"T\" to generate the error plots       #\n",
    "#                       generated with the learned errors from the             #\n",
    "#                       learnErrors() function.                                #\n",
    "# - pool                If pool = \"TRUE\", the algorithm will pool together all #\n",
    "#                       samples prior to sample inference. If pool = \"FALSE\",  #\n",
    "#                       sample inference is performed on each sample           #\n",
    "#                       individually. If pool = \"pseudo\", the algorithm will   #\n",
    "#                       perform pseudo-pooling between individually processed  #\n",
    "#                       samples.                                               #\n",
    "# - chimeras            If \"T\" The samples in a sequence table are             #\n",
    "#                       independently checked for bimeras, and a consensus     #\n",
    "#                       decision on each sequence variant is made.             #\n",
    "# - extra_params        Any extra parameter belonging to dada2's function      #\n",
    "#                       dada(). e.g., \"selfConsist=FALSE\".                     #\n",
    "#                       The value passed through this variable is send directly#\n",
    "#                       to the function in R. Therefore, if your extra_params  #\n",
    "#                       involves more than one argument, separate them with    #\n",
    "#                       commas, as the example above.                          #                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "dada2_asv:\n",
    "  generateErrPlots: \"T\"\n",
    "  pool: \"pseudo\"\n",
    "  cpus: 10\n",
    "  chimeras : \"T\"\n",
    "  extra_params: \"selfConsist=FALSE\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# ASV_WF:                             Assign taxonomy                          #\n",
    "# rule: run_dada2                                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "# The taxonomy assignation for the ASVs is performed within the dada2 package, #    \n",
    "# by using the assignTaxonomy() function which uses the RDP Naive Bayesian     #\n",
    "# Classifier algorithm.                                                        #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - db             Full path to reference database training files:             #\n",
    "#                  https://benjjneb.github.io/dada2/training.html              #\n",
    "# - extra_params   Any extra parameter belonging to dada2's function           #\n",
    "#                  assignaxonomy(). e.g., \"minBoot=45, tryRC=TRUE\".            #\n",
    "#                  The value passed through this variable is send directly to  #\n",
    "#                  the function in R. Therefore, if your extra_params involves #\n",
    "#                  more than one argument, separate them with commas, as the   #\n",
    "#                  example above.                                              #\n",
    "# - add_sp         Arguments for assigning genus-species binomials to the input#\n",
    "#                  sequences by exact matching against a reference fasta using #\n",
    "#                  the addSpecies() function from dada2.                       #\n",
    "#   + add          \"T\" or \"F\" Try to assign or not genus-species binomials.    #\n",
    "#   + db_sps       If add = \"T\"  full path to the reference species file.      #\n",
    "#   + extra_params Any extra parameter belonging to dada2's function           #\n",
    "#                  addSpecies(). e.g., \"allowMultiple=TRUE\".                   #\n",
    "#                  The value passed through this variable is send directly to  #\n",
    "#                  the function in R. Therefore, if your extra_params involves #\n",
    "#                  more than one argument, separate them with commas, as the   #\n",
    "#                  example above.                                              #\n",
    "# NOTE: Find available databases at:                                           # \n",
    "# https://benjjneb.github.io/dada2/training.html                               #\n",
    "# Files \"silva_nr_v132_train_set.fa.gz\" & \"silva_species_assignment_v132.fa.gz\"#\n",
    "# are just a suggestion and they must be downloaded from previous reference    #\n",
    "# prior to its use. rename {FULL_PATH} to the path were your files were        #\n",
    "# downloaded.                                                                  #  \n",
    "#------------------------------------------------------------------------------#\n",
    "dada2_taxonomy:\n",
    "  db: \"/export/data01/databases/silva/r138/dada2/silva_nr_v138_train_set.fa.gz\"\n",
    "  extra_params: \"minBoot=45\"\n",
    "  add_sps: \n",
    "    add: \"T\"\n",
    "    db_sps: \"/export/data01/databases/silva/r138/dada2/silva_species_assignment_v138.fa.gz\"\n",
    "    extra_params: \"allowMultiple=TRUE\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                 Align reads vs a reference database                  #\n",
    "# rule: align_vs_reference                                                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "# In some cases you may want to align the reads against a reference database   #\n",
    "# before generating OTUs. This facilitates removing technical sequence\t       #\n",
    "# (primers, adapters) at the beginning and/or end of the reads, filters\t       #\n",
    "# potential chimeric sequences or sequences of no interest, which the primers  #\n",
    "# amplified but which are not part of the study (e.g. reads of human origin).  #\n",
    "# Therefore, this alignment step can improve the OTU clustering and further    #\n",
    "# taxonomy assignation. In order to run the alignment here, use 'align': \"T\"   #\n",
    "# and bear in mind that whenever you do this, you should only do it with small #\n",
    "# to medium sized data bases, because sequence alignment is computationally    #\n",
    "# costly.                                       \t    \t\t       #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - mothur_cmd:      Enter the command to call mothur [default: \"mothur\"].     #\n",
    "# - align:           \"T\" or \"F\" to run or skip this rule respectively.         #\n",
    "# - dbAligned:       Database to perform the alignment with.                   #\n",
    "# - cpus:            Number of CPUs to perform the alignment with.             #\n",
    "#------------------------------------------------------------------------------#\n",
    "align_vs_reference:\n",
    "  mothur_cmd: \"mothur\"\n",
    "  align: \"F\"\n",
    "  dbAligned: \"\"\n",
    "  cpus: 4\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:             Remove adapters / primers                                #\n",
    "# rule: cutadapt                                                               #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule runs Cutadapt. Cutadapt searches for adapters in the reads and     #\n",
    "# removes them when it finds any. Unless you use a filtering option, all       #\n",
    "# reads that were present in the input file will also be present in the output #\n",
    "# file, some of them trimmed, some of them not.                                #\n",
    "# For details, see: http://cutadapt.readthedocs.io/en/stable/guide.html        #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - cutAdapters  if T (true) remove adapters, if F do not remove adapters      #\n",
    "#                [default: F].                                                 #\n",
    "# - command      Necessary command to invoke cutadapt [default: \"cutadapt\"].   #\n",
    "# - adapters     \"-a ADAPTER\" Remove adapters at the 3' end of the             #\n",
    "#                sequence. Cutadapt deals with 3' adapters by removing the     #\n",
    "#                adapter and any sequence that may follow. Add the \"$\"         # \n",
    "#                character to the end of an adapter sequence in order to anchor#\n",
    "#                the adapter to the end of the read, such as \"-a ADAPTER$\". The#\n",
    "#                adapter will only be removed if it is a suffix of the read.   #\n",
    "#                \"-g ADAPTER\" Removes adapters at the 5' end. If you want to   #\n",
    "#                trim only if the sequence starts with the adapter, use        #\n",
    "#                \"-g ^ADAPTER\". The \"^\" character indicates that the adapter is#\n",
    "#                'anchored' at the beginning of the read. In other words: The  #\n",
    "#                adapter is expected to be a prefix of the read.               #\n",
    "#                If your sequence of interest is 'framed' by a 5' and a 3'     #\n",
    "#                adapter and you want to remove both adapters, then you may    #\n",
    "#                want to use a linked adapter. A linked adapter combines an    #\n",
    "#                anchored 5' adapter and a 3' adapter. The 3' adapter can be   #\n",
    "#                regular or anchored. The idea is that a read is only trimmed  #\n",
    "#                if the anchored adapters occur. Thus, the 5' adapter is always# \n",
    "#                required, and if the 3' adapter was specified as anchored, it #\n",
    "#                also must exist for a successful match, e.g.                  #\n",
    "#                -a GTGYCAGCMGCCGCGGTAA...ATTAGAWACCCVNGTAGTCC                 #\n",
    "# - extra_params Any extra parameter. A useful extra parameter here is         #\n",
    "#                --match-read-wildcards which interprets IUPAC wildcards in    #\n",
    "#                reads or -O to ensure the minimum overlap between a read and  #\n",
    "#                one adapter to be found.                                      # \n",
    "# More info and options at http://cutadapt.readthedocs.io/en/stable/guide.html #\n",
    "#------------------------------------------------------------------------------#\n",
    "cutAdapters: \"F\"\n",
    "cutadapt:\n",
    "  command: \"cutadapt\"\n",
    "  adapters: \"\"\n",
    "  extra_params: \"--match-read-wildcards  -O 19\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                 Identify chimeric sequences                          #\n",
    "# rule: search_chimera                                                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule will be executed if and only if the option 'search' is set to \"T\"! #\n",
    "# Chimeric sequences are predicted using usearch61. This algorithm performs    #\n",
    "# both de novo (abundance based) chimera and reference based detection.        #\n",
    "# Unclustered sequences are used as input rather than a representative sequence#\n",
    "# set, as the sequences will be clustered to get abundance data.               #\n",
    "# The results are all input sequences not flagged as chimeras.                 # \n",
    "# This rule implements different methods for identifying chimeras, below       #\n",
    "# more details about the available options.                                    #\n",
    "# For details about usearch, see: http://drive5.com/usearch/usearch_docs.html  #\n",
    "# If you are using usearch61, bear in mind that you can use a reference        #\n",
    "# database via extra_params, e.g., \"-r /path/to/gold_db/gold.fa\".              #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - method        Select the method for chimera identification:                #\n",
    "#                  - \"usearch61\" This algorithm performs both de novo          #\n",
    "#                    (abundance based) chimera and reference based detection.  #\n",
    "#                    This method uses the usearch implementation within qiime's#\n",
    "#                    script identify_chimeric_seqs.py.                         #\n",
    "#                    To use reference based detection supply the reference     #\n",
    "#                    database via extra_params, e.g.:\"-r /dbs/gold_db/gold.fa\" #\n",
    "#                 For details, see: http://drive5.com/usearch/usearch_docs.html#\n",
    "#                  - \"uchime_denovo\" detect chimeras de novo (uses vsearch)    #\n",
    "#                  - \"uchime_ref\" detect chimeras using a reference database.  #\n",
    "#                     In this later case, user MUST suply extra_params:        #\n",
    "#                     \"--db </full/path/to/db.fasta>\" (i.e., path to gold db). #\n",
    "# - threads       Number of threads to use.                                    #\n",
    "# - extra_params  Any extra parameter. It is recommended to run the chimeric   #\n",
    "#                 search against a chimera database, e.g.,                     #\n",
    "#                 \"-r /export/data/databases/gold_db/gold.fa\" for \"usearch61\"  #\n",
    "#                 or \"--db /export/data/databases/gold_db/gold.fa\" for         #\n",
    "#                 \"uchime_ref\".                                                #\n",
    "# - search        \"T\" | \"F\" (true or false) to execute chimera checking or not.#\n",
    "#------------------------------------------------------------------------------#\n",
    "chimera:\n",
    "  search: \"F\"\n",
    "  method: \"uchime_ref\"\n",
    "  threads: 10\n",
    "  extra_params: \"--db /{FULL_PATH}/gold.fa\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:           Length filtering for OTU or ASV                           #\n",
    "# rule: remove_short_long_reads                                                #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule executes a script in order to filter the reads based on their      #\n",
    "# length.                                                                      #\n",
    "# First, this script generates a histogram based on the read lengths           #\n",
    "# distribution. Next, this histogram is used by the same script in different   # \n",
    "# ways depending on the pipeline's execution mode (interactive or automatic).  #\n",
    "# If the pipeline is executed in \"Interactive\" mode, it will always stop at    #\n",
    "# this step and let the user choose between the following options:             #\n",
    "#      * Use the values specified in the configuration file (the ones from the #\n",
    "#        parameters specified here, 'longs' and 'shorts').                     #\n",
    "#      * Filter the reads based on the median of the sequence length           #\n",
    "#        distribution +/- an offset value.                                     #\n",
    "#      * Do not filter any sequence.                                           #\n",
    "#      * Stop the pipeline.                                                    #\n",
    "# If the pipeline is executed in non-interactive/automatic mode, the pipeline  #\n",
    "# will not stop and the reads will be filtered according to the                #\n",
    "# 'non_interactive_behaviour' value.                                           #\n",
    "# - non_interactive_behaviour  Behaviour for the non-interactive/automatic     # \n",
    "#                              mode. Valid options are:                        #  \n",
    "#                              * CFG: use the values from the configuration    #\n",
    "#                                     file ('longs' and 'shorts').             #\n",
    "#                              * AVG: use the values from the median           #\n",
    "#                                     distribution.                            #           \n",
    "#                              * NONE: do not filter any read.                 #\n",
    "#                              [default: CFG]                                  #\n",
    "# - offset                     value used to determine the bounds for the      #\n",
    "#                              filtering when 'interactive' is set to 'F' and  #\n",
    "#                              non_interactive_behaviour is equal to 'AVG'.    # \n",
    "#                              [default: 10].                                  #\n",
    "# - longs                      Maximum read length.                            #\n",
    "# - shorts                     Minimum read length                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "# REMARK: If your library contains more than one expected fragment length, you #\n",
    "# can either:                                                                  #\n",
    "# A) Do not filter any length.                                                 #\n",
    "# B) Use inclusive boundaries.                                                 #\n",
    "# C) Rerun the pipeline for all the expected fragment lengths, e.g.,           #\n",
    "#    \"--forcerun remove_short_long_reads\". In this case, do not forget to      #\n",
    "#    backup previous results, otherwise, they will be overwritten.             #\n",
    "#------------------------------------------------------------------------------#\n",
    "rm_reads:\n",
    "  non_interactive_behaviour: \"AVG\"\n",
    "  offset: 10\n",
    "  longs: 220\n",
    "  shorts: 260\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                       Dereplicate                                    #\n",
    "# rule: dereplicate, pick_derep_representatives                                #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This parameters allows the user to dereplicate the sequences over the FULL   #\n",
    "# LENGTH (100% identity) before applying any OTU picking strategy. This is     #\n",
    "# advised for very large datasets, when OTU picking methods take too long or   #\n",
    "# have memory issues.                                                          #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - dereplicate    Dereplicate sequences over their full length (F/T)          #\n",
    "#                  [default: \"F\"].                                             #\n",
    "# - vsearch_cmd    Command for calling vsearch [default: \"vsearch\"].           #\n",
    "# - min_abundance  Minimum abundance for output from dereplication.            #\n",
    "# - strand:        plus|both, search \"plus\" or \"both\" strands                  #\n",
    "#                  [default: \"both\"].                                          #\n",
    "# - extra_params   Dereplication is performed using vsearch, you can add       #\n",
    "#                  different options described by vsearch --help.              #\n",
    "#------------------------------------------------------------------------------#\n",
    "derep:\n",
    "  dereplicate: \"T\"\n",
    "  vsearch_cmd: \"vsearch\"\n",
    "  min_abundance: 1\n",
    "  strand: \"both\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                       OTU picking                                    #\n",
    "# rule: cluster_OTUs                                                           #\n",
    "#------------------------------------------------------------------------------#\n",
    "# The OTU picking step assigns similar sequences to operational taxonomic      #\n",
    "# units, or OTUs, by clustering sequences based on a user-defined similarity   #\n",
    "# threshold. Sequences which are similar at or above the threshold level are   #\n",
    "# taken to represent the presence of a taxonomic unit (e.g., approximately at  #\n",
    "# genus level, when the similarity threshold is set at 0.94) in the sequence   #\n",
    "# collection. Swarm takes a different clustering approach which does not       #\n",
    "# require setting a threshold. Instead, clusters are formed using sequence     # \n",
    "# graphs and abundance.                                                        #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - s              Sequence similarity threshold between 0 and 1. This applies #\n",
    "#                  for the following methods 'm': uclust, uclust_ref, usearch, #\n",
    "#                  usearch_ref, usearch61,  usearch61_ref, sumaclust, and      #\n",
    "#                  sortmerna. [default:0.97].                                  #\n",
    "# - m              OTU picking method. Valid choices are: sortmerna, mothur,   #\n",
    "#                  trie, uclust, uclust_ref, usearch, usearch_ref, swarm,      #\n",
    "#\t\t           cdhit, sumaclust, prefix_suffix.                    \t       #\n",
    "# - extra_params   Any extra parameter. Run 'pick_otus.py -h' to see all       #\n",
    "#                  options.                                                    # \n",
    "#------------------------------------------------------------------------------#\n",
    "pickOTU:\n",
    "  s: \"0.97\"\n",
    "  m: \"uclust\"\n",
    "  cpus: \"6\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                Select representative sequences                       #\n",
    "# rule: pick_representatives                                                   #\n",
    "#------------------------------------------------------------------------------#\n",
    "# After picking OTUs, this rule picks a representative sequence for each OTU.  #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - m             Method for picking representative sequences. Valid choices   #\n",
    "#                 are: random, longest, most_abundant, first                   #\n",
    "#                 [default: \"most_abundant\"].                                  # \n",
    "#                 Note: \"first\" chooses the cluster seed when picking otus with#\n",
    "#                 uclust.                                                      #\n",
    "# - extra_params  Any extra parameter. Run 'pick_rep_set.py -h' to see all     #\n",
    "#                 options.                                                     #\n",
    "#------------------------------------------------------------------------------#\n",
    "pickRep:\n",
    "  m: \"most_abundant\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "################################################################################\n",
    "# OTU_WF:                    Assign taxonomy                                   #\n",
    "# rule: assign_taxonomy                                                        #\n",
    "#------------------------------------------------------------------------------#\n",
    "# Performs taxonomy assignment for the representative sequences.               #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# This step can be performed using three different tools (parameter 'tool'):   #\n",
    "# 1) VSEARCH.  Compare target sequences 'db_file' to the query sequences to    #\n",
    "#              assign taxonomy, using global pairwise alignment.               #\n",
    "# 2) BLAST.    This uses BLAST+.                                               #\n",
    "# 3) QIIME.    In this case, CASCABEL runs the assign_taxonomy.py script which #\n",
    "#              can use any of the following methods:                           #\n",
    "#  3.1) BLAST.  To use blast via QIIME, use \"blast\" as 'method' below.         # \n",
    "#               This uses the old version of BLAST (not BLAST+).               #\n",
    "#               This method can work with either a BLAST database, setting     #\n",
    "#               'dbType' to \"-b\" and 'dbFile' with the full path to a BLAST    #\n",
    "#               database, or with a fasta file with 'dbTYpe' set to \"-r\" and   #\n",
    "#               'dbFile' pointing to the fasta file (full path must be used).  #\n",
    "#  3.2) UCLUST. A method based on sequence clustering. To use this method type #\n",
    "#               \"uclust\" as 'method' below. This method ONLY works with        #\n",
    "#               'dbType' set to \"-r\" and 'dbFile' full path to a fasta file.   #\n",
    "#  3.3) RDP.    The Ribosomal Database Project (RDP) Classifier, a naive       #\n",
    "#               Bayesian classifier, can rapidly and accurately classify       #\n",
    "#               bacterial 16S rRNA sequences. To use this method, use \"rdp\" as #\n",
    "#               'method' below.                                                #\n",
    "#               NOTE: In order to run RDP it is necessary to provide the       #\n",
    "#               classifier path. To do so, please include the following line   #\n",
    "#               on the \"extra_params\":                                         #\n",
    "#               '--rdp_classifier_fp /path/to/rdp_classifier-2.2.jar'          #\n",
    "#               Due to some compatibility issues, RDP needs a custom fasta file#\n",
    "#               and taxonomy mapping file. You can find the appropriate files  # \n",
    "#               at the following references:                                   #\n",
    "#               'dbFile' \"/gg/gg_13_8_otus/rep_set/97_otus.fasta\"              #\n",
    "#               'mappFile' \"/gg/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt\"     #\n",
    "#            These 3 different methods can use different sets of options in    #\n",
    "#            order to fine tune the taxonomy assignment. These options can be  #\n",
    "#            realized with the extra_params parameter. To see what options are #\n",
    "#            available for each 'method', type on the command line             #\n",
    "#            'parallel_assign_taxonomy_<method>.py -h'.                        #\n",
    "# - tool     Which tool to use, options are \"vsearch\", \"qiime\" and \"blast\".    #\n",
    "# ----blast parameters                                                         #\n",
    "#    - command         Command to invoke blastn.  [default: \"blastn\"].         #\n",
    "#    - blast_db        Full path to target BLAST database for the assignation. #\n",
    "#    - fasta_db        Full path to a fasta file with sequences for the        #\n",
    "#                      assignation. In case of providing both (a Blast database#\n",
    "#                      and a fasta file, Cascabel will give priority to the    #\n",
    "#                      database 'blast_db'.                                    #\n",
    "#    - mapFile         Mapping file between sequence accessions and their      #\n",
    "#                      taxonomy. This file should be a two column file, tab    #\n",
    "#                      separated with the accessions in the first column and   #\n",
    "#                      the taxonomy in the second column with the taxonomic    #\n",
    "#                      levels separated by 'taxo_separator', e.g.              #\n",
    "#                      'AY190.44  Bacteria;Firmicutes;Bacilli;...'             # \n",
    "#    - taxo_separator  Character used to split taxonomic levels specified at   #\n",
    "#                      the taxonomy mapping file 'mapFile'. [default: \"';'\"]   #\n",
    "#    - max_target_seqs Maximum number of target sequences per sequence query.  #\n",
    "#                      All the target sequences are taking into account and    #\n",
    "#                      their taxonomies are mapped to their Lowest Common      #\n",
    "#                      Ancestor (LCA).                                         #\n",
    "#    - jobs            Number of cpus.                                         #\n",
    "#    - identity        Minimum percentage of identity (between 0 and 1).       #\n",
    "#    - extra_params    Any extra parameter. Run 'blastn -help' to see all      #\n",
    "#                      options.                                                #\n",
    "# ----vsearch parameters                                                       #\n",
    "#    - command         Command to invoke vsearch  [default: \"fastqc\"].         #\n",
    "#    - db_file         Full path to a fasta file with sequences for the        #\n",
    "#                      assignation.                                            #\n",
    "#    - mapFile \t   Mapping file between sequence accessions and their          #\n",
    "#                      taxonomy. This file should be a two column file, tab    #\n",
    "#                      separated with the accessions in the first column and   #\n",
    "#                      the taxonomy in the second column with the taxonomic    # \n",
    "#                      levels separated by the 'taxo_separator', e.g.          #\n",
    "#                      'AY190.44  Bacteria;Firmicutes;Bacilli;...'             #\n",
    "#    - taxo_separator  Character used to split taxonomic levels\tspecified at   #\n",
    "#                      the taxonomy mapping file 'mapFile'. [default: \"';'\"]   #\n",
    "#    - max_target_seqs Maximum number of target sequences per sequence query.  #\n",
    "#                      All the target sequences are taking into account\tand   #\n",
    "#                      their taxonomies\tare mapped to their Lowest Common      #\n",
    "#                      Ancestor\t(LCA).                                        #\n",
    "#    - jobs            Number of cpus.                                         #\n",
    "#    - identity        Minimum percentage of identity (between 0 and 1).       #\n",
    "#    - identity_definition  Identity definitions available by vsearch. Values  #\n",
    "#                      accepted are:                                           #\n",
    "#                      \"0\". CD-HIT definition: (matching columns) / (shortest  #\n",
    "#                      sequence length).                                       #\n",
    "#                      \"1\". Edit distance: (matching columns) / (alignment     #\n",
    "#                      length).                                                #\n",
    "#                      \"2\". Edit distance excluding terminal gaps.             #\n",
    "#                      \"3\". Marine Biological Lab definition counting each     #\n",
    "#                      extended gap (internal or terminal) as a single         #\n",
    "#                      difference: 1.0 - [(mismatches + gaps)/(longest sequence#\n",
    "#                      length)].                                               #\n",
    "#                      \"4\". BLAST definition, equivalent to --iddef 2 in a     #\n",
    "#                      context of global pairwise alignment.                   #\n",
    "#                      [default:  \"2\"].                                        #\n",
    "#    - extra_params    Any extra parameter. Run 'vsearch -h' to see all        #\n",
    "#                      options [default: \"--top_hits_only --maxrejects 32\"]    #\n",
    "# ----qiime parameters                                                         #\n",
    "#    - method          Assignation method. Choose one of: \"blast\", \"uclust\" or #\n",
    "#                      \"rdp\".                                                  #\n",
    "#    - mapFile         Path to tab-delimited file mapping sequences to assigned#\n",
    "#                      taxonomy. Each assigned taxonomy is provided as a       #\n",
    "#                      semicolon-separated list. For assignment with \"rdp\",    #\n",
    "#                      each assigned taxonomy must be exactly 6 levels deep.   #\n",
    "#                      The default mapping file supplied by Qiime can be       #\n",
    "#                      located at cascabel's environment (if created with      #\n",
    "#                      conda), in such case you may found the file at:         #\n",
    "#                      /../.conda/envs/cascabel/lib/python2.7/site-packages/ \\ #\n",
    "#                      qiime_default_reference/gg_13_8_otus/taxonomy/ \\        #\n",
    "#                      97_otu_taxonomy.txt.                                    #\n",
    "#    - dbFile          Full path to reference fasta file or blast database     #\n",
    "#                      file.                                                   #\n",
    "#                      The default fasta file supplied by Qiime can be located #\n",
    "#                      at Cascabel's environment, if created with conda, in    #\n",
    "#                      such case you may find the file at: \t                   #\n",
    "#                      /../.conda/envs/cascabel/lib/python2.7/site-packages/ \\ #\n",
    "#                      qiime_default_reference/gg_13_8_otus/taxonomy/ \\        #\n",
    "#                      97_otus.fasta\"                                          #\n",
    "#    - dbType          If 'dbFile' points to a fasta file use \"-r\". If 'dbFile'#\n",
    "#                      points to a blast database use \"-b\".                    #\n",
    "#    - jobs:           Number of jobs to start in parallel.                    #\n",
    "#------------------------------------------------------------------------------#\n",
    "assignTaxonomy:\n",
    "  tool: \"vsearch\"\n",
    "  blast:\n",
    "    command: \"blastn\"\n",
    "    blast_db: \"\"\n",
    "    fasta_db: \"\"\n",
    "    mapFile: \"\"\n",
    "    taxo_separator: \"';'\" \n",
    "    evalue: \"0.001\"\n",
    "    max_target_seqs: 5\n",
    "    jobs: 10\n",
    "    identity: 0.5   \n",
    "    extra_params: \"\"\n",
    "  vsearch:\n",
    "    command: \"vsearch\"\n",
    "    db_file: \"/{FULL_PATH}/silva132_99.fna\" #available at https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_132_release.zip\n",
    "    mapFile: \"/{FULL_PATH}/taxonomy_7_levels.txt\" #available at https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_132_release.zip\n",
    "    identity: 0.7\n",
    "    taxo_separator: \"';'\" \n",
    "    max_target_seqs: 5 \n",
    "    identity_definition: 2  \n",
    "    jobs: 10\n",
    "    extra_params: \"--top_hits_only --maxrejects 32\"\n",
    "  qiime:\n",
    "    method: \"uclust\"\n",
    "    mapFile: \"97_otu_taxonomy.txt.\"\n",
    "    dbFile: \"97_otus.fasta\"\n",
    "    dbType: \"-r\"\n",
    "    jobs: 10\n",
    "    extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# OTU_WF:                       Make OTU Table                                 #\n",
    "# rule: make_otu_table                                                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# The rule tabulates the number of times an OTU is found in each sample, and   #\n",
    "# adds the taxonomic predictions for each OTU in the last column if a taxonomy #\n",
    "# file is supplied.                                                            #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - extra_params  Any extra desired parameter. Run 'make_otu_table.py --help'  #\n",
    "#                 to see all the options.                                      #\n",
    "#------------------------------------------------------------------------------#\n",
    "makeOtu:\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                 Filter OTUs / ASV from table                        #\n",
    "# rule: filter_otu                                                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule filters the OTU / ASV table in order to remove singletons.         #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - n             The minimum total observation count of an OTU for that OTU   #\n",
    "#                 to be retained [default: \"2 ]                                #\n",
    "# - extra_params  Any extra desired parameter. Run                             #\n",
    "#                 'filter_otus_from_otu_table.py --help' to see all options.   #\n",
    "#------------------------------------------------------------------------------#\n",
    "filterOtu:\n",
    "  n: \"2\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                      Biom convert                                   #\n",
    "# rule: convert_table                                                          #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule converts the OTU biom table into a tab separated OTU table.        #\n",
    "# - tableType     [\"OTU table\"|\"Pathway table\"|\"Function table\"|\"Ortholog      #\n",
    "#                 table\"|\"Gene table\"|\"Metabolite table\"|\"Taxon table\"|\"Table\"]#\n",
    "#                 [default:  \"--table-type 'OTU table'\"].                      #\n",
    "# - headerKey     The observation metadata to include from the input BIOM table#\n",
    "#                 file when creating a tsv table file. [default: \"--header-key # \n",
    "#                 taxonomy\"].                                                  #\n",
    "# - outFormat     \"--to-tsv\": Output as tab-separated (classic) table.         #\n",
    "#                 \"--to-json\"  and \"--to-hdf5\" are also available.             #\n",
    "# - extra_params: Any extra desired parameter. Run 'biom convert --help' to see#\n",
    "#                 all the options.                                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "biom:\n",
    "  command: \"biom\"\n",
    "  tableType: \"--table-type 'OTU table'\"\n",
    "  headerKey: \"--header-key taxonomy\"\n",
    "  outFormat: \"--to-tsv\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                  summarize taxonomy                                 #\n",
    "# rule: summarize_taxa                                                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule provides summary information of the representation of taxonomic    #\n",
    "# groups within each sample. It takes an OTU / ASV table that contains         #\n",
    "# taxonomic information as input. By default, the relative abundance of each   #\n",
    "# taxonomic group will be reported, but the raw counts can be returned if \"-a\" #\n",
    "# is passed.                                                                   #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - extra_params: Any extra desired parameter. Run 'summarize_taxa.py --help'  #\n",
    "#                 to see all the options.                                      #\n",
    "#                                                                              #\n",
    "# The taxonomic levels to summarize are by default: \"--level 2,3,4,5,6\".       #\n",
    "# The meaning of this level will depend on the format of the taxon strings that#\n",
    "# are returned from the taxonomy assignment step. The taxonomy strings that are#\n",
    "# most useful are those that standardize the taxonomic level with the depth in #\n",
    "# the taxonomic strings. For instance: Level 1 = Kingdom, Level 2 = Phylum,    #\n",
    "# Level 3 = Class, Level 4 = Order, Level 5 = Family, Level 6 = Genus,         #\n",
    "# Level 7 = Species.                                                           #\n",
    "#------------------------------------------------------------------------------#\n",
    "summTaxa:\n",
    "  extra_params: \"--level 2,3,4,5,6,7\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                 Filter fasta file                                   #\n",
    "# rule: filter_rep_seqs                                                        #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule performs OTU/ASV table-based filtering: Keep all sequences that    #\n",
    "# occur in an OTU/ASV table.                                                   #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - extra_params  Any extra desired parameter. Run 'filter_fasta.py --help' to #\n",
    "#                 see all options.                                             #\n",
    "#------------------------------------------------------------------------------#\n",
    "filterFasta:\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                     Align representative sequences                  #\n",
    "# rule: align_rep_seqs                                                         #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule aligns sequences in a fasta file to each other or to a template    #\n",
    "# sequence alignment, depending on the method chosen.                          #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - align          [T/F] The user can skip this rule by setting 'align' to \"F\" #\n",
    "#                  [default: \"T\"].                                             #\n",
    "# -m               Method for aligning sequences. Valid choices are: \"pynast\", #\n",
    "#                  \"infernal\", \"clustalw\", \"muscle\", \"mafft\".                  #\n",
    "#                  [default: \"pynast\"].                                        #        \n",
    "# - extra_params:  Any extra parameter. Run 'align_seqs.py --help' to see all  #\n",
    "#                  options.                                                    #\n",
    "#------------------------------------------------------------------------------#\n",
    "alignRep:\n",
    "  align: \"F\"\n",
    "  m: \"pynast\"\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                  Filter Alignment                                   #\n",
    "# rule: filter_alignment                                                       #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule will remove positions which are gaps in every sequence.            #\n",
    "# Additionally, the user can supply a lanemask file, that defines which        #\n",
    "# positions should be included when building the tree, and which should be     #\n",
    "# ignored.                                                                     #\n",
    "# This rule, only runs if [alignRep][align] is equal to true \"T\".              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - extra_params: Any extra desired parameter. Run 'filter_alignment.py --help'# \n",
    "#                 to see all options.                                          #\n",
    "#------------------------------------------------------------------------------#\n",
    "filterAlignment:\n",
    "  extra_params: \"\"\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# BOTH_WF:                  make phylogeny                                     #\n",
    "# rule: make_tree                                                              #\n",
    "#------------------------------------------------------------------------------#\n",
    "# This rule produces a phylogenetic tree from a multiple sequence alignment.   # \n",
    "# This rule, only runs if [alignRep][align] is equal to true (\"T\").            #\n",
    "#                                                                              #\n",
    "#-----------------------------       PARAMS       -----------------------------#\n",
    "#                                                                              #\n",
    "# - method:       Method for generating the tree. Valid choices are:           #\n",
    "#                 \"clustalw\", \"raxml_v730\", \"muscle\", \"fasttree\", \"clearcut\".  # \n",
    "#                 [default: \"fasttree\"].                                       #\n",
    "# - extra_params: Any extra parameter. Run 'make_phylogeny.py --help' to see   #\n",
    "#                 all options.                                                 #\n",
    "#------------------------------------------------------------------------------#\n",
    "makeTree:\n",
    "  method: \"fasttree\"\n",
    "  extra_params: \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Create the DAG (directed acyclic graph) for your run. \n",
    "# Send it to a svg file called ASV_WF_dada2.svg\n",
    "! snakemake --configfile config.asv.double_bc.NIOZ101.yaml --dag | dot -Tsvg > ASV_WF_dada2.svg\n",
    "# A DAG is a visual representation of the entire pipeline that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule dada2Filter:\r\n",
      "    input: BlackSea2016/runs/18S/NIOZ101_data/demultiplexed/summary.txt\r\n",
      "    output: BlackSea2016/runs/18S/asv/filter_summary.out\r\n",
      "    jobid: 26\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/filter.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mRscript Scripts/asvFilter.R $PWD T 250 240 3 5 10 \",truncQ=2, rm.phix=TRUE\" BlackSea2016/runs/18S/asv/filter_summary.out BlackSea2016/runs/18S/NIOZ101_data/demultiplexed/summary.txt \u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule validate_dada2Filter:\r\n",
      "    input: BlackSea2016/runs/18S/asv/filter_summary.out\r\n",
      "    output: BlackSea2016/runs/18S/asv/filter_summary.validation.txt\r\n",
      "    jobid: 28\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule run_dada2:\r\n",
      "    input: BlackSea2016/runs/18S/NIOZ101_data/demultiplexed/summary.txt, BlackSea2016/runs/18S/asv/filter_summary.validation.txt\r\n",
      "    output: BlackSea2016/runs/18S/asv/stats_dada2.txt, BlackSea2016/runs/18S/asv/representative_seq_set.fasta, BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_tax_assignments.txt, BlackSea2016/runs/18S/asv/dada2_asv_table.txt\r\n",
      "    jobid: 21\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/dada2.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mRscript Scripts/asvDada2.R $PWD pseudo  10 T  selfConsist=FALSE BlackSea2016/runs/18S/asv/  260  220 10 T  /export/data01/databases/silva/r138/dada2/silva_nr_v138_train_set.fa.gz /export/data01/databases/silva/r138/dada2/silva_species_assignment_v138.fa.gz  T \"minBoot=45\"  10 0  F  \"allowMultiple=TRUE\" BlackSea2016/runs/18S/NIOZ101_data/demultiplexed/summary.txt BlackSea2016/runs/18S/asv/filter_summary.validation.txt\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule asv_table:\r\n",
      "    input: BlackSea2016/runs/18S/asv/dada2_asv_table.txt\r\n",
      "    output: BlackSea2016/runs/18S/asv/asv_table.txt\r\n",
      "    jobid: 27\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mcat BlackSea2016/runs/18S/asv/dada2_asv_table.txt | awk '{if(NR==1){header=\"#OTU_ID\";for(i=1;i<=NF;i++){header=header\"\\t\"$i};print header}else{print $0}}'|   awk '{ for (i=1; i<=NF; i++){ a[NR,i] = $i } } NF>p { p = NF } END { for(j=1; j<=p; j++) { str=a[1,j]; for(i=2; i<=NR; i++){ str=str\"\\t\"a[i,j]; } print str } }' > BlackSea2016/runs/18S/asv/asv_table.txt\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule asv_tax_table:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_tax_assignments.txt, BlackSea2016/runs/18S/asv/asv_table.txt\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.txt\r\n",
      "    jobid: 19\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/taxonomy_dada2/dada2.table.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mcat BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_tax_assignments.txt | awk 'NR==FNR{if(NR>1){tax=$2;for(i=3;i<=NF;i++){tax=tax\";\"$i};h[$1]=tax;}next;} {if(FNR==1){print $0\"\\ttaxonomy\"}else{print $0\"\\t\"h[$1]}}' -  BlackSea2016/runs/18S/asv/asv_table.txt > BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.txt\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule asv_to_biom:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.txt\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom\r\n",
      "    jobid: 10\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/taxonomy_dada2/dada2.biom.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mbiom convert -i BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.txt -o BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom --table-type \"OTU table\" --to-hdf5 --process-obs-metadata taxonomy \u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule filter_asv:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom\r\n",
      "    jobid: 22\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_nosingletons.bio.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mfilter_otus_from_otu_table.py -i BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom -o BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom -n 2 \u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule count_samples_final:\r\n",
      "    input: BlackSea2016/runs/18S/asv/filter_summary.out, BlackSea2016/metadata/sampleList_mergedBarcodes_NIOZ101.txt\r\n",
      "    output: BlackSea2016/runs/18S/NIOZ101_data/seqs_fw_rev_filtered.dist.txt\r\n",
      "    jobid: 18\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S, sample=NIOZ101\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mcat BlackSea2016/runs/18S/asv/filter_summary.out | awk 'NR==FNR{if(NR>1){h[$1]=$2;}next}{if(FNR>1){print $1\"\t\"h[$1]}}' - BlackSea2016/metadata/sampleList_mergedBarcodes_NIOZ101.txt > BlackSea2016/runs/18S/NIOZ101_data/seqs_fw_rev_filtered.dist.txt\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule convert_filter_asv:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.txt\r\n",
      "    jobid: 20\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_nosingletons.txt.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mbiom convert -i BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom -o BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.txt --table-type 'OTU table' --header-key taxonomy --to-tsv \u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule summarize_taxa:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/summary/asvTable_L6.txt\r\n",
      "    jobid: 14\r\n",
      "    benchmark: BlackSea2016/runs/18S/otu/taxonomy_vsearch/summary/summarize_taxa.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33msummarize_taxa.py -i BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom -o BlackSea2016/runs/18S/asv/taxonomy_dada2/summary/ --level 2,3,4,5,6,7\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule krona_report:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.txt\r\n",
      "    output: BlackSea2016/runs/18S/report_files/krona_report.dada2.html\r\n",
      "    jobid: 11\r\n",
      "    benchmark: BlackSea2016/runs/18S/otu/taxonomy_vsearch/krona_report.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule filter_rep_seqs:\r\n",
      "    input: BlackSea2016/runs/18S/asv/representative_seq_set.fasta, BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_noSingletons.fasta\r\n",
      "    jobid: 12\r\n",
      "    benchmark: BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_noSingletons.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mfilter_fasta.py -f BlackSea2016/runs/18S/asv/representative_seq_set.fasta -o BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_noSingletons.fasta -b BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom \u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule summarize_taxa_no_singletons:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom\r\n",
      "    output: BlackSea2016/runs/18S/asv/taxonomy_dada2/summary_noSingletons/asvTable_noSingletons_L6.txt\r\n",
      "    jobid: 15\r\n",
      "    benchmark: BlackSea2016/runs/18S/otu/taxonomy_vsearch/summary/summarize_taxa.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33msummarize_taxa.py -i BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable_noSingletons.biom -o BlackSea2016/runs/18S/asv/taxonomy_dada2/summary_noSingletons/ --level 2,3,4,5,6,7\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule create_sample_log:\r\n",
      "    input: BlackSea2016/runs/18S/NIOZ101_data/seqs_fw_rev_filtered.dist.txt\r\n",
      "    output: BlackSea2016/runs/18S/samples.log\r\n",
      "    jobid: 13\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule report_all_asv:\r\n",
      "    input: BlackSea2016/runs/18S/asv/taxonomy_dada2/asvTable.biom, BlackSea2016/runs/18S/asv/taxonomy_dada2/summary/asvTable_L6.txt, BlackSea2016/runs/18S/asv/taxonomy_dada2/summary_noSingletons/asvTable_noSingletons_L6.txt, BlackSea2016/runs/18S/asv/taxonomy_dada2/representative_seq_set_noSingletons.fasta, BlackSea2016/runs/18S/report_files/krona_report.dada2.html, BlackSea2016/runs/18S/samples.log\r\n",
      "    output: BlackSea2016/runs/18S/reporttmp_all.html\r\n",
      "    jobid: 7\r\n",
      "    benchmark: BlackSea2016/runs/18S/report_all.benchmark\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule tune_report_all:\r\n",
      "    input: BlackSea2016/runs/18S/reporttmp_all.html\r\n",
      "    output: BlackSea2016/runs/18S/asv_report_dada2.html\r\n",
      "    jobid: 4\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule distribution_chart:\r\n",
      "    input: BlackSea2016/runs/18S/NIOZ101_data/seqs_fw_rev_filtered.dist.txt\r\n",
      "    output: BlackSea2016/runs/18S/report_files/seqs_fw_rev_filtered.NIOZ101.dist.png\r\n",
      "    jobid: 9\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S, sample=NIOZ101\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mpython  Scripts/sampleDist.manual.py BlackSea2016/runs/18S/NIOZ101_data/seqs_fw_rev_filtered.dist.txt BlackSea2016/runs/18S/report_files/seqs_fw_rev_filtered.NIOZ101.dist.png\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule report:\r\n",
      "    input: BlackSea2016/runs/18S/asv_report_dada2.html, BlackSea2016/runs/18S/report_files/seqs_fw_rev_filtered.NIOZ101.dist.png, BlackSea2016/runs/18S/NIOZ101_data/demultiplexed/summary.txt\r\n",
      "    output: BlackSea2016/runs/18S/NIOZ101_data/report.html\r\n",
      "    jobid: 6\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S, sample=NIOZ101\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule tune_report:\r\n",
      "    input: BlackSea2016/runs/18S/NIOZ101_data/report.html\r\n",
      "    output: BlackSea2016/runs/18S/report_NIOZ101_dada2.html\r\n",
      "    jobid: 3\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S, sample=NIOZ101\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule translate_to_pdf:\r\n",
      "    input: BlackSea2016/runs/18S/asv_report_dada2.html\r\n",
      "    output: BlackSea2016/runs/18S/asv_report_dada2.pdf\r\n",
      "    jobid: 5\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mwkhtmltopdf  -T 10mm -B 30mm  BlackSea2016/runs/18S/asv_report_dada2.html BlackSea2016/runs/18S/asv_report_dada2.pdf\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule translate_pdf_final_report:\r\n",
      "    input: BlackSea2016/runs/18S/report_NIOZ101_dada2.html, BlackSea2016/runs/18S/asv_report_dada2.pdf\r\n",
      "    output: BlackSea2016/runs/18S/report_NIOZ101_dada2.pdf\r\n",
      "    jobid: 2\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S, sample=NIOZ101\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mwkhtmltopdf  -T 10mm -B 30mm BlackSea2016/runs/18S/report_NIOZ101_dada2.html BlackSea2016/runs/18S/report_NIOZ101_dada2.pdf\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mrule create_portable_report:\r\n",
      "    input: BlackSea2016/runs/18S/report_NIOZ101_dada2.html, BlackSea2016/runs/18S/asv_report_dada2.html\r\n",
      "    output: BlackSea2016/runs/18S/report_dada2.zip\r\n",
      "    jobid: 1\r\n",
      "    wildcards: PROJECT=BlackSea2016, run=18S\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mzip -r BlackSea2016/runs/18S/report_dada2.zip BlackSea2016/runs/18S/report_files BlackSea2016/runs/18S/report_NIOZ101_dada2.html BlackSea2016/runs/18S/asv_report_dada2.html\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[32mlocalrule all:\r\n",
      "    input: BlackSea2016/runs/18S/report_NIOZ101_dada2.pdf, BlackSea2016/runs/18S/report_dada2.zip\r\n",
      "    jobid: 0\u001b[0m\r\n",
      "\u001b[32m\u001b[0m\r\n",
      "\u001b[33mJob counts:\r\n",
      "\tcount\tjobs\r\n",
      "\t1\tall\r\n",
      "\t1\tasv_table\r\n",
      "\t1\tasv_tax_table\r\n",
      "\t1\tasv_to_biom\r\n",
      "\t1\tconvert_filter_asv\r\n",
      "\t1\tcount_samples_final\r\n",
      "\t1\tcreate_portable_report\r\n",
      "\t1\tcreate_sample_log\r\n",
      "\t1\tdada2Filter\r\n",
      "\t1\tdistribution_chart\r\n",
      "\t1\tfilter_asv\r\n",
      "\t1\tfilter_rep_seqs\r\n",
      "\t1\tkrona_report\r\n",
      "\t1\treport\r\n",
      "\t1\treport_all_asv\r\n",
      "\t1\trun_dada2\r\n",
      "\t1\tsummarize_taxa\r\n",
      "\t1\tsummarize_taxa_no_singletons\r\n",
      "\t1\ttranslate_pdf_final_report\r\n",
      "\t1\ttranslate_to_pdf\r\n",
      "\t1\ttune_report\r\n",
      "\t1\ttune_report_all\r\n",
      "\t1\tvalidate_dada2Filter\r\n",
      "\t23\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "## 13. make a dry-run to see the commands \n",
    "! snakemake --configfile config.asv.double_bc.NIOZ101.yaml -np\n",
    "\n",
    "## 14. The execution plan contains 49 rules (see bottom line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Run the pipeline\n",
    "`screen -S cascabel`  \n",
    "`snakemake --configfile config.asv.double_bc.NIOZ101.yaml`\n",
    "\n",
    "**you cannot continue in Jupyter**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### messages during running the pipeline\n",
    "#### 1.\n",
    "```Python\n",
    "selfConsist step 2\n",
    "Please enter the option which fits better for your data:\n",
    "1. Use values from configuration file: length >  260  and length <  220\n",
    "2. Use values from median + /- the offset: length >  350.718998862344  and length <  370.718998862344\n",
    "3. Specify new values\n",
    "4. Print sequence length histogram.\n",
    "5. Interrupt workflow.\n",
    "Enter your option:\n",
    "\n",
    "```\n",
    "First option 4 (print histogram)  \n",
    "Check for lenghts with >20 reads  \n",
    "Choose option 3 (Specify new values) and choose  \n",
    "shortest with >20 reads -1 (349)  \n",
    "longest with >20 reads +1 (370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once that your run is finished, locate the report “report_dada2.zip” \n",
    "transfer the report_.zip to your computer   \n",
    "[C:\\Users\\dnalab\\Desktop\\unix_workshop\\Cascabel\\report_dada2.zip]\n",
    "\n",
    "exit server  \n",
    "\n",
    "go to directory  \n",
    "`! cd /home/mobaxterm/Desktop/unix_workshop/Cascabel`   \n",
    "Un-compress the file and inspect the report  \n",
    "`! unzip report_dada2.zip`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
